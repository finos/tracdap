/*
 * Copyright 2020 Accenture Global Solutions Limited
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";
package trac.api;

option java_package = "com.accenture.trac.api";
option java_multiple_files = true;

import "trac/metadata/type.proto";
import "trac/metadata/object_id.proto";
import "trac/metadata/data.proto";
import "trac/metadata/file.proto";
import "trac/metadata/object.proto";
import "trac/metadata/tag.proto";
import "trac/metadata/search.proto";

import "google/api/annotations.proto";


/**
 * Public API for creating, updating, reading and querying primary data stored in the TRAC platform.
 *
 * TODO
 */
service TracDataApi {

  /**
   * Create a new dataset, supplying the schema and content as a data stream
   *
   * This method creates a new dataset and a corresponding DATA object
   * in the TRAC metadata store. Once a dataset is created it can be used as an
   * input into a model run, it can also be read and queried using the data API.
   * Data can be supplied in any format supported by the platform.
   *
   * *Although large datasets can be uploaded using this call, data import jobs
   * are normally used to bring in large volumes of data from external systems.*
   *
   * The request must specify a schema for the dataset, incoming data will be
   * verified against the schema. Schemas can be specified using either:
   *
   *    * A full schema definition - if a full schema is supplied, it will be embedded
   *      with the dataset and used for this dataset only
   *    * A schema ID - a tag selector for an existing SCHEMA object, which may be
   *      shared by multiple datasets
   *
   * The "format" parameter describes the format used to upload data. For example,
   * to upload a CSV file the format would be set to "text/csv" and the file content
   * can be uploaded directly, or to upload the output of an editor grid in a web
   * client the format can be set to "text/json" to upload a JSON representation of
   * the editor contents. TRAC will apply format conversion before the data is
   * processed and stored.
   *
   * Tag updates can be supplied to tag the newly created dataset, they behave exactly
   * the same as tag updates in the createObject() call of TracMetadataApi.
   *
   * This is a client streaming method. The first message in the request stream
   * must contain all the request fields and metadata, including a schema specifier.
   * After the first message all metadata fields should be omitted. Subsequent messages
   * should contain the content of the dataset as a series of chunks, encoded as per
   * the "format" field of the first message. Clients may choose whether or not to
   * include a chunk of data in the first message and empty (i.e. zero-length) chunks
   * are permitted at any point in the stream.
   *
   * This method returns the header of the newly created DATA object. Error conditions
   * include: Invalid request, unknown tenant, schema not found (if an external schema
   * ID is used), format not supported, data does not match schema, corrupt or invalid
   * data stream. Storage errors may also be reported if there is a problem communicating
   * with the underlying storage technology. In the event of an error, TRAC will do its
   * best to clean up any partially-written data in the storage layer.
   */
  rpc createDataset (stream DataWriteRequest) returns (metadata.TagHeader);

  /**
   * Create a new dataset, supplying the schema and content as a single blob
   *
   * This method creates a new dataset and a corresponding DATA object
   * in the TRAC metadata store. Once a dataset is created it can be used as an
   * input into a model run, it can also be read and queried using the data API.
   * Data can be supplied in any format supported by the platform.
   *
   * The request must specify a schema for the dataset, incoming data will be
   * verified against the schema. Schemas can be specified using either:
   *
   *    * A full schema definition - if a full schema is supplied, it will be embedded
   *      with the dataset and used for this dataset only
   *    * A schema ID - a tag selector for an existing SCHEMA object, which may be
   *      shared by multiple datasets
   *
   * The "format" parameter describes the format used to upload data. For example,
   * to upload a CSV file the format would be set to "text/csv" and the file content
   * can be uploaded directly, or to upload the output of an editor grid in a web
   * client the format can be set to "text/json" to upload a JSON representation of
   * the editor contents. TRAC will apply format conversion before the data is
   * processed and stored.
   *
   * Tag updates can be supplied to tag the newly created dataset, they behave exactly
   * the same as tag updates in the createObject() call of TracMetadataApi.
   *
   * This is a unary call, all the request fields and metadata (including schema specifier)
   * and dataset content encoded as per the "format" field are supplied in a single message.
   * It is intended for working with small datasets and for use in environments where client
   * streaming is not available (particularly in gRPC-Web clients).
   *
   * This method returns the header of the newly created DATA object. Error conditions
   * include: Invalid request, unknown tenant, schema not found (if an external schema
   * ID is used), format not supported, data does not match schema, corrupt or invalid
   * data stream. Storage errors may also be reported if there is a problem communicating
   * with the underlying storage technology. In the event of an error, TRAC will do its
   * best to clean up any partially-written data in the storage layer.
   */
  rpc createSmallDataset (DataWriteRequest) returns (metadata.TagHeader);

  /**
   * Update an existing dataset, supplying the schema and content as a data stream
   *
   * This method updates an existing dataset and the corresponding DATA object
   * in the TRAC metadata store. As per the TRAC immutability guarantee, the original
   * version of the dataset is not altered. After an update, both the original version
   * and the new version are available to use as inputs into a model runs and to read
   * and query using the data API. Data can be supplied in any format supported by the
   * platform.
   *
   * *Although large datasets can be uploaded using this call, data import jobs
   * are normally used to bring in large volumes of data from external systems.*
   *
   * To update a dataset, the priorVersion field must indicate the dataset being updated.
   * Only the latest version of a dataset can be updated.
   *
   * The request must specify a schema for the new version of the dataset, incoming data
   * will be verified against the schema. The new schema must be compatible with the schema
   * of the previous version. Schemas can be specified using either:
   *
   *    * A full schema definition - Datasets created using an embedded schema must supply
   *      a full schema for all subsequent versions and each schema version must be compatible
   *      with the version before. Fields may be added, but not removed or altered.
   *    * A schema ID - Datasets created using an external schema must use the same external
   *      schema ID for all subsequent versions. It is permitted for later versions of a
   *      dataset to use later versions of the external schema, but not earlier versions.
   *
   * The "format" parameter describes the format used to upload data. For example,
   * to upload a CSV file the format would be set to "text/csv" and the file content
   * can be uploaded directly, or to upload the output of an editor grid in a web
   * client the format can be set to "text/json" to upload a JSON representation of
   * the editor contents. It is not necessary for different versions of the same dataset
   * to be uploaded using the same format. TRAC will apply format conversion before the
   * data is processed and stored.
   *
   * Tag updates can be supplied to tag the new version of the dataset, they behave exactly
   * the same as tag updates in the updateObject() call of TracMetadataApi.
   *
   * This is a client streaming method. The first message in the request stream
   * must contain all the request fields and metadata, including a schema specifier.
   * After the first message all metadata fields should be omitted. Subsequent messages
   * should contain the content of the dataset as a series of chunks, encoded as per
   * the "format" field of the first message. Clients may choose whether or not to
   * include a chunk of data in the first message and empty (i.e. zero-length) chunks
   * are permitted at any point in the stream.
   *
   * This method returns the header of the version of the DATA object. Error conditions
   * include: Invalid request, unknown tenant, schema not found (if an external schema
   * ID is used), schema version not compatible, format not supported, data does not match
   * schema, corrupt or invalid data stream. Storage errors may also be reported if there is
   * a problem communicating with the underlying storage technology. In the event of an error,
   * TRAC will do its best to clean up any partially-written data in the storage layer.
   */
  rpc updateDataset (stream DataWriteRequest) returns (metadata.TagHeader);

  /**
   * Update an existing dataset, supplying the schema and content as a single blob
   *
   * This method updates an existing dataset and the corresponding DATA object
   * in the TRAC metadata store. As per the TRAC immutability guarantee, the original
   * version of the dataset is not altered. After an update, both the original version
   * and the new version are available to use as inputs into a model runs and to read
   * and query using the data API. Data can be supplied in any format supported by the
   * platform.
   *
   * To update a dataset, the priorVersion field must indicate the dataset being updated.
   * Only the latest version of a dataset can be updated.
   *
   * The request must specify a schema for the new version of the dataset, incoming data
   * will be verified against the schema. The new schema must be compatible with the schema
   * of the previous version. Schemas can be specified using either:
   *
   *    * A full schema definition - Datasets created using an embedded schema must supply
   *      a full schema for all subsequent versions and each schema version must be compatible
   *      with the version before. Fields may be added, but not removed or altered.
   *    * A schema ID - Datasets created using an external schema must use the same external
   *      schema ID for all subsequent versions. It is permitted for later versions of a
   *      dataset to use later versions of the external schema, but not earlier versions.
   *
   * The "format" parameter describes the format used to upload data. For example,
   * to upload a CSV file the format would be set to "text/csv" and the file content
   * can be uploaded directly, or to upload the output of an editor grid in a web
   * client the format can be set to "text/json" to upload a JSON representation of
   * the editor contents. It is not necessary for different versions of the same dataset
   * to be uploaded using the same format. TRAC will apply format conversion before the
   * data is processed and stored.
   *
   * Tag updates can be supplied to tag the new version of the dataset, they behave exactly
   * the same as tag updates in the updateObject() call of TracMetadataApi.
   *
   * This is a unary call, all the request fields and metadata (including schema specifier)
   * and dataset content encoded as per the "format" field are supplied in a single message.
   * It is intended for working with small datasets and for use in environments where client
   * streaming is not available (particularly in gRPC-Web clients).
   *
   * This method returns the header of the version of the DATA object. Error conditions
   * include: Invalid request, unknown tenant, schema not found (if an external schema
   * ID is used), schema version not compatible, format not supported, data does not match
   * schema, corrupt or invalid data stream. Storage errors may also be reported if there is
   * a problem communicating with the underlying storage technology. In the event of an error,
   * TRAC will do its best to clean up any partially-written data in the storage layer.
   */
  rpc updateSmallDataset (DataWriteRequest) returns (metadata.TagHeader);

  /**
   * Read the contents of an existing dataset
   *
   * This method reads the contents of an existing dataset, returning both the schema
   * and the content in the requested format. Data can be requested in any format
   * supported by the platform.
   *
   * The request uses a regular TagSelector to indicate which dataset and version to read.
   * The format parameter is a mime type and must be a supported data format.
   *
   * This is a server streaming method. The first message in the response stream will
   * contain a schema definition for the dataset (this may come from an embedded schema
   * or an external schema object). The second and subsequent messages will deliver the
   * content of the dataset in the requested format. TRAC guarantees that the first message
   * will always contain an empty chunk of content, which can be safely ignored.
   *
   * Error conditions include: Invalid request, unknown tenant, object not found, format
   * not supported. Storage errors may also be reported if there is a problem communicating
   * with the underlying storage technology.
   */
  rpc readDataset (DataReadRequest) returns (stream DataReadResponse);

  /**
   * Upload a new file into TRAC
   *
   * Calling this method will create a new FILE object in the metadata store.
   * Tag updates can be supplied when creating a FILE, they will be passed on to the
   * metadata service. The semantics for tag updates are identical to the createObject()
   * method in TracMetadataApi.
   *
   * This is a client streaming method. The first message in the request stream
   * must contain all the request fields and required metadata. The second and subsequent
   * messages should contain the content of the file as a series of chunks (byte buffers).
   * (other fields that are set after the first message will be ignored).
   * Empty chunks can be included at any point in the stream and will be ignored.
   * Clients may choose to include the first chunk in the first message along with the
   * request metadata, or to put an empty chunk in the first message and start streaming
   * content in the second message. For very small files, it is possible to put the entire
   * content in one chunk in the first message, so there is only a single message in the stream.
   * All of these approaches are supported.
   *
   * Clients may specify the size of the file being created. When a size is supplied, TRAC
   * will check the size against the number of bytes stored. If the stored file size does not
   * match the supplied value, the error will be reported with an error status of DATA_LOSS.
   * When no size is supplied the check cannot be performed.
   *
   * The method returns the header of the newly created FILE object. Error conditions
   * include: Invalid request, unknown tenant and validation failure and data loss
   * (if the number of bytes stored does not match the number specified in the request).
   * Storage errors may also be reported if there is a problem communicating with the
   * underlying storage technology. In the event of an error, TRAC will do its best to
   * clean up any partially-written data in the storage layer.
   */
  rpc createFile (stream FileWriteRequest) returns (metadata.TagHeader);

  rpc createSmallFile (FileWriteRequest) returns (metadata.TagHeader);

  /**
   * Upload a new version of an existing file into TRAC
   *
   * Calling this method will update the relevant FILE object in the metadata store.
   * The latest version of the FILE must be supplied in the priorVersion field
   * of the request. For example if the latest version of a FILE object is version 2,
   * the priorVersion field should refer to version 2 and TRAC will create version 3
   * as a result of the update call. The metadata and content of prior versions
   * remain unaltered. The file name may be changed between versions, but the extension
   * and mime type must stay the same. Tag updates can be supplied when updating a FILE,
   * they will be passed on to the metadata service> The semantics for tag updates are
   * identical to the updateObject() method in TracMetadataApi.
   *
   * This is a client streaming method. The first message in the request stream
   * must contain all the request fields and required metadata. The second and subsequent
   * messages should contain the content of the file as a series of byte buffers.
   * (other fields that are set after the first message will be ignored).
   * Empty chunks can be included at any point in the stream and will be ignored.
   * Clients may choose to include the first chunk in the first message along with the
   * request metadata, or to put an empty chunk in the first message and start streaming
   * content in the second message. For very small files, it is possible to put the entire
   * content in one chunk in the first message, so there is only a single message in the stream.
   * All of these approaches are supported.
   *
   * Clients may specify the size of the file being updated. When a size is supplied, TRAC
   * will check the size against the number of bytes stored. If the stored file size does not
   * match the supplied value, the error will be reported with an error status of DATA_LOSS.
   * When no size is supplied the check cannot be performed.
   *
   * The call returns the header for the new version of the FILE object. Error conditions
   * include: Invalid request, unknown tenant, validation failure, failed preconditions
   * (e.g. extension and mime type changes) and data loss (if the number of bytes stored
   * does not match the number specified in the request). Storage errors may also be reported
   * if there is a problem communicating with the underlying storage technology. In the event
   * of an error, TRAC will do its best to clean up any partially-written data in the storage layer.
   */
  rpc updateFile (stream FileWriteRequest) returns (metadata.TagHeader);

  rpc updateSmallFile (FileWriteRequest) returns (metadata.TagHeader);

  /**
   * Download a file that has been stored in TRAC
   *
   * The request uses a regular TagSelector to indicate which file to read. The
   * semantics of the request are identical to the readObject() method in
   * TracMetadataApi.
   *
   * This is a server streaming method. The first message in the response stream will
   * contain the response metadata (i.e. the file definition). The second
   * and subsequent messages will deliver the content of the file as a stream of chunks
   * (byte buffers). Empty chunks may be included at any point in the stream and
   * should be ignored. In particular, TRAC guarantees that the chunk in the first
   * message will always be an empty chunk. Clients are free to ignore this chunk,
   * for example if they have a separate function for processing the first message in
   * the response stream. Alternatively clients may process the empty chunk in the firs
   * message in the same way as any other chunk. Both approaches are supported.
   *
   * Error conditions include: Invalid request, unknown tenant, unknown object ID,
   * object type does not match ID, unknown object version, unknown tag version.
   * Storage errors may also be reported if there is a problem communicating with the
   * underlying storage technology.
   */
  rpc readFile (FileReadRequest) returns (stream FileReadResponse);

}


message DataWriteRequest {

  /**
   * Tenant code for the requested operation, always required
   */
  string tenant = 1;

  /**
   * Prior object/tag version to use for update operations
   *
   * This field should be omitted when creating a new dataset.
   */
  optional metadata.TagSelector priorVersion = 2;

  /**
   * Tag update operations to be applied
   *
   * Tag updates are applied in exactly the same way as for metadata write requests.
   *
   * @see MetadataWriteRequest
   * @see TracMetadataApi
   */
  repeated metadata.TagUpdate tagUpdates = 3;


  oneof schemaDefinition {
    metadata.TagSelector schemaId = 4;
    metadata.SchemaDefinition schema = 5;
  }

  // Alternatives to schemaId or schema
  reserved 6, 7, 8;
  reserved "inferSchema";
  reserved "schemaUpdates";
  reserved "schemaHandling";

  string format = 9;

//  map<string, metadata.Value> formatOptions = 10;
//  optional metadata.PartKey part = 11;
//  optional bool deltaUpdate = 12;

  bytes content = 1000;
}

message DataReadRequest {

  string tenant = 1;
  metadata.TagSelector selector = 2;

  string format = 3;

//  map<string, metadata.Value> formatOptions = 4;
//  optional metadata.PartKey part = 5;
//  optional uint64 offset = 6;
//  optional uint32 limit = 7;
}

message DataReadResponse {

  optional metadata.SchemaDefinition schema = 1;

  bytes content = 1000;
}

message DataQueryRequest {

  string tenant = 1;

  string query = 2;
  map<string, metadata.TagSelector> targets = 3;

  string format = 4;
}


/**
 * Request to create or update a FILE and store content to the platform
 *
 * @see TracDataApi.createFile()
 * @see TracDataApi.updateFile()
 */
message FileWriteRequest {

  /**
   * Tenant code for the requested operation, always required
   */
  string tenant = 1;

  /**
   * Prior object/tag version to use for update operations
   *
   * This field should be omitted when creating a new file.
   */
  optional metadata.TagSelector priorVersion = 2;

  /**
   * Tag update operations to be applied
   *
   * Tag updates are applied in exactly the same way as for metadata write requests.
   *
   * @see MetadataWriteRequest
   * @see TracMetadataApi
   */
  repeated metadata.TagUpdate tagUpdates = 3;

  /**
   * File name of the file being saved, always required
   *
   * Must be a valid file name with no path component (i.e. the name may not contain slashes).
   *
   * If the name contains a period character, the file extension will be set as the
   * portion of the name following the last period. Otherwise the file extension will be blank.
   *
   * For update requests the name may change but the extension must stay the same. E.g. a file
   * can be created as "my_file_v1.txt" and updated as "my_file_v2.txt", but updating with the
   * name as "my_file_v1.doc" would not be allowed. Attempting to change the extension in an
   * update will result in an error status of FAILED_PRECONDITION.
   *
   * Fle name and extension are stored in the file definition, and in the trac_file_name
   * and trac_file_extension attributes.
   */
  string name = 4;

  /**
   * Mime type of the file being saved, always required
   *
   * Must be a valid mime type. For update requests, the mime type must match exactly
   * with the mime type supplied when the file was originally created. Attempting to change
   * the mime type in an update will result in an error status of FAILED_PRECONDITION.
   *
   * Mime type is stored in the file definition and in the trac_file_mime_type attribute.
   */
  string mimeType = 5;

  /**
   * Size of the file being saved, if known in advance
   *
   * When a size is supplied, TRAC will check the size against the number of bytes stored.
   * If the stored file size does not match the supplied value, the error will be reported
   * with an error status of DATA_LOSS. When no size is supplied the check cannot be performed.
   *
   * File size is stored in the created file definition and in the trac_file_size attribute.
   */
  optional uint64 size = 6;

  /**
   * A chunk of the file content
   *
   * The file content should be sent as a stream of chunks (byte buffers), with one chunk in each message.
   * Empty chunks can be included at any point in the request stream and will be ignored.
   */
  bytes content = 1000;
}


/**
 * Request to read a FILE, i.e. retrieve its content from the platform
 *
 * @see TracDataApi
 */
message FileReadRequest {

  /**
   * Tenant code for the requested operation, always required
   */
  string tenant = 1;

  /**
   * Selector for the FILE being read
   */
  metadata.TagSelector selector = 2;
}


/**
 * Response to a request to read a FILE, i.e. retrieve its content from the platform
 *
 * @see TracDataApi.readFile()
 */
message FileReadResponse {

  /**
   * Definition of the FILE being read
   */
  optional metadata.FileDefinition fileDefinition = 1;

  /**
   * A chunk of the file content
   *
   * The file content will be sent as a stream of chunks (byte buffers), with one chunk in each message.
   * Empty chunks may be included at any point in the response stream and should be ignored.
   * In particular, TRAC guarantees that the chunk in the first message will always be an empty chunk.
   */
  bytes content = 1000;
}
